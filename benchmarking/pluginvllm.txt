Benchmarking vLLM endpoint: http://localhost:8000/v1
Model: meta-llama/Llama-3.1-8B-Instruct
Number of requests: 50
Concurrent requests: 10
Max tokens: 256
Streaming: True


================================================================================
BENCHMARK RESULTS
================================================================================

Total Requests: 50
Successful: 50
Failed: 0

Total Time: 43.30s
Total Tokens Generated: 16,026

--------------------------------------------------------------------------------
THROUGHPUT (tokens/second)
--------------------------------------------------------------------------------
Average:  37.05 tok/s
P50:      37.18 tok/s
P95:      38.95 tok/s
P99:      39.62 tok/s

--------------------------------------------------------------------------------
TIME TO FIRST TOKEN (TTFT)
--------------------------------------------------------------------------------
Average:  79.11 ms
P50:      68.18 ms
P95:      103.57 ms
P99:      105.69 ms

--------------------------------------------------------------------------------
TOTAL REQUEST LATENCY
--------------------------------------------------------------------------------
Average:  8.652s
P50:      8.607s
P95:      8.819s
P99:      8.822s

--------------------------------------------------------------------------------
OTHER METRICS
--------------------------------------------------------------------------------
Avg Tokens per Request: 320.5
Avg Time per Token: 27.020 ms
Requests per Second: 1.15
Requests with Probe Values: 0/50 (0.0%)

================================================================================

