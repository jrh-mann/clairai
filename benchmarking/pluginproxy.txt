Benchmarking vLLM endpoint: http://localhost:6969/v1
Model: meta-llama/Llama-3.1-8B-Instruct
Number of requests: 50
Concurrent requests: 10
Max tokens: 256
Streaming: True


================================================================================
BENCHMARK RESULTS
================================================================================

Total Requests: 50
Successful: 50
Failed: 0

Total Time: 43.47s
Total Tokens Generated: 16,212

--------------------------------------------------------------------------------
THROUGHPUT (tokens/second)
--------------------------------------------------------------------------------
Average:  37.33 tok/s
P50:      37.39 tok/s
P95:      39.88 tok/s
P99:      40.94 tok/s

--------------------------------------------------------------------------------
TIME TO FIRST TOKEN (TTFT)
--------------------------------------------------------------------------------
Average:  108.41 ms
P50:      105.07 ms
P95:      152.04 ms
P99:      157.25 ms

--------------------------------------------------------------------------------
TOTAL REQUEST LATENCY
--------------------------------------------------------------------------------
Average:  8.686s
P50:      8.689s
P95:      8.729s
P99:      8.732s

--------------------------------------------------------------------------------
OTHER METRICS
--------------------------------------------------------------------------------
Avg Tokens per Request: 324.2
Avg Time per Token: 26.830 ms
Requests per Second: 1.15
Requests with Probe Values: 50/50 (100.0%)

================================================================================

