Benchmarking vLLM endpoint: http://localhost:8000/v1
Model: meta-llama/Llama-3.1-8B-Instruct
Number of requests: 50
Concurrent requests: 10
Max tokens: 256
Streaming: True


================================================================================
BENCHMARK RESULTS
================================================================================

Total Requests: 50
Successful: 50
Failed: 0

Total Time: 42.35s
Total Tokens Generated: 16,166

--------------------------------------------------------------------------------
THROUGHPUT (tokens/second)
--------------------------------------------------------------------------------
Average:  38.20 tok/s
P50:      38.48 tok/s
P95:      40.34 tok/s
P99:      41.05 tok/s

--------------------------------------------------------------------------------
TIME TO FIRST TOKEN (TTFT)
--------------------------------------------------------------------------------
Average:  79.01 ms
P50:      73.00 ms
P95:      101.50 ms
P99:      101.83 ms

--------------------------------------------------------------------------------
TOTAL REQUEST LATENCY
--------------------------------------------------------------------------------
Average:  8.464s
P50:      8.465s
P95:      8.505s
P99:      8.506s

--------------------------------------------------------------------------------
OTHER METRICS
--------------------------------------------------------------------------------
Avg Tokens per Request: 323.3
Avg Time per Token: 26.222 ms
Requests per Second: 1.18
Requests with Probe Values: 0/50 (0.0%)

================================================================================

